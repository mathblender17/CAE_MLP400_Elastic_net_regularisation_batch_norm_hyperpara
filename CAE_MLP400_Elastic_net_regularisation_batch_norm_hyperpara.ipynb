{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO0wD1FFAoSB"
      },
      "source": [
        "**APPROACH 1: Elastic regularisation applied indirectly & Grid Searc for hyper parameters** ***only l1 used***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "xg-xIm3vgbvE",
        "outputId": "693adbf8-6b63-479b-ab0f-c91abd11b77e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 4s 0us/step\n",
            "1563/1563 [==============================] - 22s 14ms/step\n",
            "313/313 [==============================] - 3s 10ms/step\n",
            "Batch Size: 8, Learning Rate: 0.001, Test Accuracy: 0.2390\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7f29c63319ce>\u001b[0m in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mearly_stopping_monitor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         mlp_history = mlp.fit(x_train_split, y_train_split,\n\u001b[0m\u001b[1;32m     72\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1854\u001b[0m                             \u001b[0mpss_evaluation_shards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pss_evaluation_shards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m                         )\n\u001b[0;32m-> 1856\u001b[0;31m                     val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1857\u001b[0m                         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m                         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2294\u001b[0m                         ):\n\u001b[1;32m   2295\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2296\u001b[0;31m                             logs = test_function_runner.run_step(\n\u001b[0m\u001b[1;32m   2297\u001b[0m                                 \u001b[0mdataset_or_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2298\u001b[0m                                 \u001b[0mdata_handler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(self, dataset_or_iterator, data_handler, step, unused_shards)\u001b[0m\n\u001b[1;32m   4106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_or_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_shards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m         \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_or_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    878\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize and convert to float32\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Define the input shape\n",
        "input_img = Input(shape=(32, 32, 3))\n",
        "\n",
        "# Encoder part of the CAE\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "# Flatten the encoded output for MLP\n",
        "flatten = Flatten()(encoded)\n",
        "\n",
        "# Create a model for encoding features\n",
        "encoder_model = Model(input_img, flatten)\n",
        "\n",
        "# Obtain encoded features for training and validation sets\n",
        "flatten_train_np = encoder_model.predict(x_train)\n",
        "flatten_test_np = encoder_model.predict(x_test)\n",
        "\n",
        "# Hyperparameters\n",
        "batch_sizes = [8, 16, 32, 64]\n",
        "learning_rates = [0.001, 0.01]\n",
        "l1_ratio = 0.5  # Elastic Net mixing parameter\n",
        "\n",
        "# Results storage\n",
        "best_accuracy = 0\n",
        "best_params = {}\n",
        "\n",
        "# Grid search over hyperparameters\n",
        "for batch_size in batch_sizes:\n",
        "    for lr in learning_rates:\n",
        "        # MLP for recognition\n",
        "        mlp = Sequential([\n",
        "            Dense(400, activation='relu', input_shape=(flatten_train_np.shape[1],)),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.5),\n",
        "            Dense(64, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.5),\n",
        "            Dense(10, activation='softmax', kernel_regularizer=regularizers.l1_l2(l1=l1_ratio, l2=l1_ratio))\n",
        "        ])\n",
        "\n",
        "        # Compile the MLP\n",
        "        optimizer = Adam(learning_rate=lr)\n",
        "        mlp.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        # Train the MLP on the encoded features\n",
        "        x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(\n",
        "            flatten_train_np, y_train, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        early_stopping_monitor = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
        "        mlp_history = mlp.fit(x_train_split, y_train_split,\n",
        "                              epochs=100,\n",
        "                              batch_size=batch_size,\n",
        "                              validation_data=(x_val_split, y_val_split),\n",
        "                              callbacks=[early_stopping_monitor],\n",
        "                              verbose=0)\n",
        "\n",
        "        # Evaluate MLP on test set\n",
        "        test_loss, test_accuracy = mlp.evaluate(flatten_test_np, y_test, verbose=0)\n",
        "\n",
        "        # Store results\n",
        "        if test_accuracy > best_accuracy:\n",
        "            best_accuracy = test_accuracy\n",
        "            best_params = {'Batch Size': batch_size, 'Learning Rate': lr}\n",
        "\n",
        "        print(f'Batch Size: {batch_size}, Learning Rate: {lr}, Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "print(f'Best Accuracy: {best_accuracy:.4f} with Batch Size: {best_params[\"Batch Size\"]}, Learning Rate: {best_params[\"Learning Rate\"]}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItsytLMRTbXT"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "170498071/170498071 [==============================] - 2s 0us/step\n",
        "1563/1563 [==============================] - 7s 3ms/step\n",
        "313/313 [==============================] - 1s 4ms/step\n",
        "Batch Size: 8, Learning Rate: 0.001, Test Accuracy: 0.2378\n",
        "Batch Size: 8, Learning Rate: 0.01, Test Accuracy: 0.2418\n",
        "Batch Size: 16, Learning Rate: 0.001, Test Accuracy: 0.2594\n",
        "Batch Size: 16, Learning Rate: 0.01, Test Accuracy: 0.2388\n",
        "Batch Size: 32, Learning Rate: 0.001, Test Accuracy: 0.2368\n",
        "Batch Size: 32, Learning Rate: 0.01, Test Accuracy: 0.2548\n",
        "Batch Size: 64, Learning Rate: 0.001, Test Accuracy: 0.0969\n",
        "Batch Size: 64, Learning Rate: 0.01, Test Accuracy: 0.2800\n",
        "Best Accuracy: 0.2800 with Batch Size: 64, Learning Rate: 0.01\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2AL4E82BQx1"
      },
      "source": [
        "**Approach 2 :  *l1 and l2 both used***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-TXz_SSDYHR"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.regularizers import l1_l2\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6hAGt-YDcKQ"
      },
      "outputs": [],
      "source": [
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQebl8u9Dk3M"
      },
      "outputs": [],
      "source": [
        "# Normalize and convert to float32\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Define the input shape\n",
        "input_img = Input(shape=(32, 32, 3))\n",
        "\n",
        "# Encoder part of the CAE\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "# Flatten the encoded output for MLP\n",
        "flatten = Flatten()(encoded)\n",
        "\n",
        "# Create a model for encoding features\n",
        "encoder_model = Model(input_img, flatten)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taiSQYFTGog8",
        "outputId": "6b653644-4e1e-4b6d-c737-fc8c49e85152"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1563/1563 [==============================] - 3s 2ms/step\n",
            "313/313 [==============================] - 1s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "# Obtain encoded features for training and validation sets\n",
        "flatten_train_np = encoder_model.predict(x_train)\n",
        "flatten_test_np = encoder_model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B19SKf-PDnSq"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters for Elastic Net regularization\n",
        "alpha = 0.5  # Mixing parameter (trade-off between L1 and L2)\n",
        "rho = 0.5    # Ratio of L1 penalty to total penalty\n",
        "\n",
        "# Results storage\n",
        "best_accuracy = 0\n",
        "best_params = {}\n",
        "\n",
        "# Grid search over hyperparameters\n",
        "batch_sizes = [8, 16, 32, 64]\n",
        "learning_rates = [0.001, 0.01]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GVU0igBsGkg",
        "outputId": "b1207adf-802e-42da-85b9-c1c4a9be53ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "5000/5000 - 20s - loss: 2.9384 - accuracy: 0.1153 - val_loss: 2.3381 - val_accuracy: 0.1089 - 20s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "5000/5000 - 19s - loss: 2.3515 - accuracy: 0.1428 - val_loss: 2.3755 - val_accuracy: 0.1731 - 19s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "5000/5000 - 20s - loss: 2.4004 - accuracy: 0.1911 - val_loss: 2.3412 - val_accuracy: 0.2317 - 20s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "5000/5000 - 19s - loss: 2.3941 - accuracy: 0.2159 - val_loss: 2.3410 - val_accuracy: 0.2414 - 19s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "5000/5000 - 20s - loss: 2.3915 - accuracy: 0.2169 - val_loss: 2.3173 - val_accuracy: 0.2430 - 20s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "5000/5000 - 18s - loss: 2.3943 - accuracy: 0.2193 - val_loss: 2.3461 - val_accuracy: 0.2356 - 18s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "5000/5000 - 19s - loss: 2.4096 - accuracy: 0.2130 - val_loss: 2.3295 - val_accuracy: 0.2232 - 19s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "5000/5000 - 19s - loss: 2.4014 - accuracy: 0.2165 - val_loss: 2.3066 - val_accuracy: 0.2443 - 19s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "5000/5000 - 19s - loss: 2.4035 - accuracy: 0.2189 - val_loss: 2.2935 - val_accuracy: 0.2284 - 19s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "5000/5000 - 19s - loss: 2.3994 - accuracy: 0.2189 - val_loss: 2.2941 - val_accuracy: 0.2553 - 19s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "5000/5000 - 18s - loss: 2.4103 - accuracy: 0.2144 - val_loss: 2.3281 - val_accuracy: 0.2430 - 18s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "5000/5000 - 19s - loss: 2.4132 - accuracy: 0.2143 - val_loss: 2.2868 - val_accuracy: 0.2513 - 19s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "5000/5000 - 19s - loss: 2.4069 - accuracy: 0.2192 - val_loss: 2.2634 - val_accuracy: 0.2630 - 19s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "5000/5000 - 20s - loss: 2.4092 - accuracy: 0.2176 - val_loss: 2.2646 - val_accuracy: 0.2775 - 20s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "5000/5000 - 19s - loss: 2.4104 - accuracy: 0.2133 - val_loss: 2.2930 - val_accuracy: 0.2252 - 19s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "5000/5000 - 19s - loss: 2.4085 - accuracy: 0.2122 - val_loss: 2.2881 - val_accuracy: 0.2448 - 19s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "5000/5000 - 19s - loss: 2.4047 - accuracy: 0.2207 - val_loss: 2.2569 - val_accuracy: 0.2703 - 19s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "5000/5000 - 19s - loss: 2.4136 - accuracy: 0.2171 - val_loss: 2.2789 - val_accuracy: 0.2510 - 19s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "5000/5000 - 19s - loss: 2.4133 - accuracy: 0.2153 - val_loss: 2.3224 - val_accuracy: 0.2424 - 19s/epoch - 4ms/step\n",
            "Epoch 20/100\n",
            "5000/5000 - 19s - loss: 2.4184 - accuracy: 0.2090 - val_loss: 2.3259 - val_accuracy: 0.2085 - 19s/epoch - 4ms/step\n",
            "Epoch 21/100\n",
            "5000/5000 - 19s - loss: 2.4092 - accuracy: 0.2133 - val_loss: 2.2571 - val_accuracy: 0.2401 - 19s/epoch - 4ms/step\n",
            "Epoch 22/100\n",
            "5000/5000 - 21s - loss: 2.4041 - accuracy: 0.2111 - val_loss: 2.2905 - val_accuracy: 0.2522 - 21s/epoch - 4ms/step\n",
            "Epoch 23/100\n",
            "5000/5000 - 19s - loss: 2.4060 - accuracy: 0.2127 - val_loss: 2.2618 - val_accuracy: 0.2312 - 19s/epoch - 4ms/step\n",
            "Epoch 24/100\n",
            "5000/5000 - 19s - loss: 2.3993 - accuracy: 0.2117 - val_loss: 2.2543 - val_accuracy: 0.2508 - 19s/epoch - 4ms/step\n",
            "Epoch 25/100\n",
            "5000/5000 - 18s - loss: 2.4043 - accuracy: 0.2062 - val_loss: 2.3103 - val_accuracy: 0.2370 - 18s/epoch - 4ms/step\n",
            "Epoch 26/100\n",
            "5000/5000 - 19s - loss: 2.4027 - accuracy: 0.2085 - val_loss: 2.3404 - val_accuracy: 0.2235 - 19s/epoch - 4ms/step\n",
            "Epoch 27/100\n",
            "5000/5000 - 19s - loss: 2.4004 - accuracy: 0.2077 - val_loss: 2.3129 - val_accuracy: 0.2546 - 19s/epoch - 4ms/step\n",
            "Epoch 28/100\n",
            "5000/5000 - 19s - loss: 2.4001 - accuracy: 0.2037 - val_loss: 2.2518 - val_accuracy: 0.2216 - 19s/epoch - 4ms/step\n",
            "Epoch 29/100\n",
            "5000/5000 - 21s - loss: 2.3944 - accuracy: 0.2052 - val_loss: 2.2750 - val_accuracy: 0.2364 - 21s/epoch - 4ms/step\n",
            "Epoch 30/100\n",
            "5000/5000 - 18s - loss: 2.3925 - accuracy: 0.2076 - val_loss: 2.2743 - val_accuracy: 0.2453 - 18s/epoch - 4ms/step\n",
            "Epoch 31/100\n",
            "5000/5000 - 19s - loss: 2.3861 - accuracy: 0.2089 - val_loss: 2.2834 - val_accuracy: 0.2468 - 19s/epoch - 4ms/step\n",
            "Epoch 32/100\n",
            "5000/5000 - 19s - loss: 2.3895 - accuracy: 0.2044 - val_loss: 2.2391 - val_accuracy: 0.2464 - 19s/epoch - 4ms/step\n",
            "Epoch 33/100\n",
            "5000/5000 - 18s - loss: 2.3906 - accuracy: 0.2028 - val_loss: 2.2474 - val_accuracy: 0.2373 - 18s/epoch - 4ms/step\n",
            "Epoch 34/100\n",
            "5000/5000 - 21s - loss: 2.4015 - accuracy: 0.1972 - val_loss: 2.3009 - val_accuracy: 0.2353 - 21s/epoch - 4ms/step\n",
            "Epoch 35/100\n",
            "5000/5000 - 18s - loss: 2.3926 - accuracy: 0.2001 - val_loss: 2.2452 - val_accuracy: 0.2556 - 18s/epoch - 4ms/step\n",
            "Epoch 36/100\n",
            "5000/5000 - 19s - loss: 2.3944 - accuracy: 0.2017 - val_loss: 2.2593 - val_accuracy: 0.2385 - 19s/epoch - 4ms/step\n",
            "Epoch 37/100\n",
            "5000/5000 - 18s - loss: 2.3919 - accuracy: 0.2006 - val_loss: 2.2192 - val_accuracy: 0.2582 - 18s/epoch - 4ms/step\n",
            "Epoch 38/100\n",
            "5000/5000 - 19s - loss: 2.3899 - accuracy: 0.2013 - val_loss: 2.2436 - val_accuracy: 0.2281 - 19s/epoch - 4ms/step\n",
            "Epoch 39/100\n",
            "5000/5000 - 18s - loss: 2.3857 - accuracy: 0.1969 - val_loss: 2.2294 - val_accuracy: 0.2498 - 18s/epoch - 4ms/step\n",
            "Epoch 40/100\n",
            "5000/5000 - 19s - loss: 2.3872 - accuracy: 0.1984 - val_loss: 2.2483 - val_accuracy: 0.2543 - 19s/epoch - 4ms/step\n",
            "Epoch 41/100\n",
            "5000/5000 - 19s - loss: 2.3832 - accuracy: 0.2031 - val_loss: 2.2474 - val_accuracy: 0.2408 - 19s/epoch - 4ms/step\n",
            "Epoch 42/100\n",
            "5000/5000 - 19s - loss: 2.3818 - accuracy: 0.1987 - val_loss: 2.2215 - val_accuracy: 0.2569 - 19s/epoch - 4ms/step\n",
            "Epoch 43/100\n",
            "5000/5000 - 19s - loss: 2.3841 - accuracy: 0.1989 - val_loss: 2.2413 - val_accuracy: 0.2430 - 19s/epoch - 4ms/step\n",
            "Epoch 44/100\n",
            "5000/5000 - 18s - loss: 2.3834 - accuracy: 0.2004 - val_loss: 2.2515 - val_accuracy: 0.2329 - 18s/epoch - 4ms/step\n",
            "Epoch 45/100\n",
            "5000/5000 - 19s - loss: 2.3785 - accuracy: 0.2020 - val_loss: 2.2674 - val_accuracy: 0.2552 - 19s/epoch - 4ms/step\n",
            "Epoch 46/100\n",
            "5000/5000 - 19s - loss: 2.3786 - accuracy: 0.2027 - val_loss: 2.2420 - val_accuracy: 0.2331 - 19s/epoch - 4ms/step\n",
            "Epoch 47/100\n",
            "5000/5000 - 19s - loss: 2.3761 - accuracy: 0.2046 - val_loss: 2.2800 - val_accuracy: 0.2266 - 19s/epoch - 4ms/step\n",
            "Batch Size: 8, Learning Rate: 0.001, Test Accuracy: 0.2348\n",
            "Epoch 1/100\n",
            "5000/5000 - 21s - loss: 2.9347 - accuracy: 0.1536 - val_loss: 2.8008 - val_accuracy: 0.1935 - 21s/epoch - 4ms/step\n",
            "Epoch 2/100\n",
            "5000/5000 - 18s - loss: 2.9581 - accuracy: 0.1733 - val_loss: 2.9748 - val_accuracy: 0.1868 - 18s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "5000/5000 - 19s - loss: 2.9955 - accuracy: 0.1903 - val_loss: 2.9087 - val_accuracy: 0.2322 - 19s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "5000/5000 - 18s - loss: 3.0225 - accuracy: 0.1948 - val_loss: 2.9236 - val_accuracy: 0.2226 - 18s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "5000/5000 - 19s - loss: 3.0093 - accuracy: 0.1988 - val_loss: 2.9426 - val_accuracy: 0.2207 - 19s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "5000/5000 - 19s - loss: 2.9971 - accuracy: 0.1938 - val_loss: 2.8652 - val_accuracy: 0.2382 - 19s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "5000/5000 - 19s - loss: 2.9896 - accuracy: 0.1936 - val_loss: 2.8987 - val_accuracy: 0.2301 - 19s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "5000/5000 - 19s - loss: 2.9814 - accuracy: 0.1935 - val_loss: 2.9288 - val_accuracy: 0.2595 - 19s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "5000/5000 - 19s - loss: 2.9995 - accuracy: 0.1821 - val_loss: 2.9994 - val_accuracy: 0.2152 - 19s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "5000/5000 - 19s - loss: 3.0018 - accuracy: 0.1907 - val_loss: 2.9590 - val_accuracy: 0.1801 - 19s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "5000/5000 - 18s - loss: 3.0011 - accuracy: 0.1855 - val_loss: 2.9231 - val_accuracy: 0.2088 - 18s/epoch - 4ms/step\n",
            "Batch Size: 8, Learning Rate: 0.01, Test Accuracy: 0.2019\n",
            "Epoch 1/100\n",
            "2500/2500 - 11s - loss: 3.4120 - accuracy: 0.1122 - val_loss: 2.3296 - val_accuracy: 0.1212 - 11s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "2500/2500 - 10s - loss: 2.3316 - accuracy: 0.1216 - val_loss: 2.3330 - val_accuracy: 0.1092 - 10s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "2500/2500 - 9s - loss: 2.3333 - accuracy: 0.1268 - val_loss: 2.3341 - val_accuracy: 0.1784 - 9s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "2500/2500 - 9s - loss: 2.3414 - accuracy: 0.1824 - val_loss: 2.3216 - val_accuracy: 0.1973 - 9s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "2500/2500 - 9s - loss: 2.3300 - accuracy: 0.2205 - val_loss: 2.2691 - val_accuracy: 0.2535 - 9s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "2500/2500 - 10s - loss: 2.3179 - accuracy: 0.2335 - val_loss: 2.2847 - val_accuracy: 0.2293 - 10s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "2500/2500 - 10s - loss: 2.3152 - accuracy: 0.2357 - val_loss: 2.2587 - val_accuracy: 0.2478 - 10s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "2500/2500 - 10s - loss: 2.3091 - accuracy: 0.2353 - val_loss: 2.2528 - val_accuracy: 0.2726 - 10s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "2500/2500 - 9s - loss: 2.3074 - accuracy: 0.2370 - val_loss: 2.2631 - val_accuracy: 0.2664 - 9s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "2500/2500 - 10s - loss: 2.3033 - accuracy: 0.2373 - val_loss: 2.2475 - val_accuracy: 0.2589 - 10s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "2500/2500 - 9s - loss: 2.3042 - accuracy: 0.2412 - val_loss: 2.2983 - val_accuracy: 0.2415 - 9s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "2500/2500 - 10s - loss: 2.3125 - accuracy: 0.2361 - val_loss: 2.2561 - val_accuracy: 0.2416 - 10s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "2500/2500 - 11s - loss: 2.3123 - accuracy: 0.2351 - val_loss: 2.2949 - val_accuracy: 0.2448 - 11s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "2500/2500 - 10s - loss: 2.3294 - accuracy: 0.2310 - val_loss: 2.2380 - val_accuracy: 0.2772 - 10s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "2500/2500 - 9s - loss: 2.3275 - accuracy: 0.2239 - val_loss: 2.1897 - val_accuracy: 0.2825 - 9s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "2500/2500 - 10s - loss: 2.3261 - accuracy: 0.2258 - val_loss: 2.2349 - val_accuracy: 0.2793 - 10s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "2500/2500 - 10s - loss: 2.3328 - accuracy: 0.2201 - val_loss: 2.3600 - val_accuracy: 0.1998 - 10s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "2500/2500 - 10s - loss: 2.3239 - accuracy: 0.2226 - val_loss: 2.2973 - val_accuracy: 0.2369 - 10s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "2500/2500 - 10s - loss: 2.3164 - accuracy: 0.2268 - val_loss: 2.2162 - val_accuracy: 0.2746 - 10s/epoch - 4ms/step\n",
            "Epoch 20/100\n",
            "2500/2500 - 9s - loss: 2.3311 - accuracy: 0.2226 - val_loss: 2.2270 - val_accuracy: 0.2525 - 9s/epoch - 4ms/step\n",
            "Epoch 21/100\n",
            "2500/2500 - 10s - loss: 2.3275 - accuracy: 0.2194 - val_loss: 2.1983 - val_accuracy: 0.2496 - 10s/epoch - 4ms/step\n",
            "Epoch 22/100\n",
            "2500/2500 - 10s - loss: 2.3262 - accuracy: 0.2239 - val_loss: 2.2266 - val_accuracy: 0.2547 - 10s/epoch - 4ms/step\n",
            "Epoch 23/100\n",
            "2500/2500 - 10s - loss: 2.3276 - accuracy: 0.2178 - val_loss: 2.2591 - val_accuracy: 0.2494 - 10s/epoch - 4ms/step\n",
            "Epoch 24/100\n",
            "2500/2500 - 10s - loss: 2.3311 - accuracy: 0.2191 - val_loss: 2.2288 - val_accuracy: 0.2476 - 10s/epoch - 4ms/step\n",
            "Epoch 25/100\n",
            "2500/2500 - 9s - loss: 2.3321 - accuracy: 0.2194 - val_loss: 2.2255 - val_accuracy: 0.2510 - 9s/epoch - 4ms/step\n",
            "Batch Size: 16, Learning Rate: 0.001, Test Accuracy: 0.2607\n",
            "Epoch 1/100\n",
            "2500/2500 - 11s - loss: 2.8320 - accuracy: 0.1619 - val_loss: 2.8102 - val_accuracy: 0.2078 - 11s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "2500/2500 - 10s - loss: 2.8939 - accuracy: 0.2108 - val_loss: 2.9017 - val_accuracy: 0.1937 - 10s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "2500/2500 - 10s - loss: 2.8906 - accuracy: 0.2177 - val_loss: 2.8009 - val_accuracy: 0.2223 - 10s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "2500/2500 - 10s - loss: 2.8910 - accuracy: 0.2193 - val_loss: 2.8628 - val_accuracy: 0.2155 - 10s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "2500/2500 - 10s - loss: 2.9035 - accuracy: 0.2170 - val_loss: 2.8799 - val_accuracy: 0.2117 - 10s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "2500/2500 - 9s - loss: 2.8985 - accuracy: 0.2202 - val_loss: 2.7896 - val_accuracy: 0.2502 - 9s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "2500/2500 - 10s - loss: 2.8961 - accuracy: 0.2193 - val_loss: 2.8072 - val_accuracy: 0.2541 - 10s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "2500/2500 - 10s - loss: 2.9146 - accuracy: 0.2152 - val_loss: 2.8881 - val_accuracy: 0.2407 - 10s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "2500/2500 - 10s - loss: 2.9103 - accuracy: 0.2224 - val_loss: 2.8133 - val_accuracy: 0.2216 - 10s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "2500/2500 - 10s - loss: 2.9038 - accuracy: 0.2150 - val_loss: 2.8064 - val_accuracy: 0.2673 - 10s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "2500/2500 - 11s - loss: 2.8778 - accuracy: 0.2010 - val_loss: 2.7685 - val_accuracy: 0.2510 - 11s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "2500/2500 - 9s - loss: 2.8771 - accuracy: 0.1999 - val_loss: 2.8408 - val_accuracy: 0.2381 - 9s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "2500/2500 - 10s - loss: 2.8711 - accuracy: 0.2061 - val_loss: 2.8273 - val_accuracy: 0.2269 - 10s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "2500/2500 - 10s - loss: 2.8528 - accuracy: 0.2057 - val_loss: 2.7819 - val_accuracy: 0.2514 - 10s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "2500/2500 - 10s - loss: 2.8718 - accuracy: 0.2113 - val_loss: 2.8359 - val_accuracy: 0.2192 - 10s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "2500/2500 - 10s - loss: 2.8736 - accuracy: 0.2061 - val_loss: 2.9953 - val_accuracy: 0.2068 - 10s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "2500/2500 - 9s - loss: 2.8821 - accuracy: 0.2075 - val_loss: 2.7513 - val_accuracy: 0.2303 - 9s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "2500/2500 - 10s - loss: 2.8623 - accuracy: 0.2088 - val_loss: 2.9428 - val_accuracy: 0.2203 - 10s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "2500/2500 - 10s - loss: 2.8688 - accuracy: 0.2007 - val_loss: 2.7676 - val_accuracy: 0.2366 - 10s/epoch - 4ms/step\n",
            "Epoch 20/100\n",
            "2500/2500 - 10s - loss: 2.8660 - accuracy: 0.2062 - val_loss: 2.6984 - val_accuracy: 0.2562 - 10s/epoch - 4ms/step\n",
            "Epoch 21/100\n",
            "2500/2500 - 9s - loss: 2.8615 - accuracy: 0.2000 - val_loss: 2.8499 - val_accuracy: 0.2100 - 9s/epoch - 4ms/step\n",
            "Epoch 22/100\n",
            "2500/2500 - 10s - loss: 2.8550 - accuracy: 0.1960 - val_loss: 2.7522 - val_accuracy: 0.2155 - 10s/epoch - 4ms/step\n",
            "Epoch 23/100\n",
            "2500/2500 - 10s - loss: 2.8400 - accuracy: 0.1966 - val_loss: 2.7534 - val_accuracy: 0.2396 - 10s/epoch - 4ms/step\n",
            "Epoch 24/100\n",
            "2500/2500 - 10s - loss: 2.8461 - accuracy: 0.1932 - val_loss: 2.7850 - val_accuracy: 0.1962 - 10s/epoch - 4ms/step\n",
            "Epoch 25/100\n",
            "2500/2500 - 10s - loss: 2.8483 - accuracy: 0.1896 - val_loss: 2.8235 - val_accuracy: 0.2242 - 10s/epoch - 4ms/step\n",
            "Epoch 26/100\n",
            "2500/2500 - 9s - loss: 2.8359 - accuracy: 0.1932 - val_loss: 2.7198 - val_accuracy: 0.2288 - 9s/epoch - 4ms/step\n",
            "Epoch 27/100\n",
            "2500/2500 - 9s - loss: 2.8394 - accuracy: 0.1901 - val_loss: 2.8053 - val_accuracy: 0.2062 - 9s/epoch - 4ms/step\n",
            "Epoch 28/100\n",
            "2500/2500 - 10s - loss: 2.8529 - accuracy: 0.1897 - val_loss: 2.8220 - val_accuracy: 0.1953 - 10s/epoch - 4ms/step\n",
            "Epoch 29/100\n",
            "2500/2500 - 10s - loss: 2.8528 - accuracy: 0.1911 - val_loss: 2.8458 - val_accuracy: 0.2336 - 10s/epoch - 4ms/step\n",
            "Epoch 30/100\n",
            "2500/2500 - 10s - loss: 2.8646 - accuracy: 0.1906 - val_loss: 2.7516 - val_accuracy: 0.2157 - 10s/epoch - 4ms/step\n",
            "Batch Size: 16, Learning Rate: 0.01, Test Accuracy: 0.2187\n",
            "Epoch 1/100\n",
            "1250/1250 - 6s - loss: 4.3464 - accuracy: 0.1355 - val_loss: 2.3273 - val_accuracy: 0.0987 - 6s/epoch - 5ms/step\n",
            "Epoch 2/100\n",
            "1250/1250 - 5s - loss: 2.3271 - accuracy: 0.1097 - val_loss: 2.3281 - val_accuracy: 0.1267 - 5s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1250/1250 - 5s - loss: 2.3273 - accuracy: 0.1209 - val_loss: 2.3263 - val_accuracy: 0.1441 - 5s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "1250/1250 - 5s - loss: 2.3273 - accuracy: 0.1163 - val_loss: 2.3275 - val_accuracy: 0.1099 - 5s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1250/1250 - 5s - loss: 2.3277 - accuracy: 0.1308 - val_loss: 2.3272 - val_accuracy: 0.0990 - 5s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1250/1250 - 5s - loss: 2.3291 - accuracy: 0.1578 - val_loss: 2.3291 - val_accuracy: 0.1593 - 5s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1250/1250 - 5s - loss: 2.3233 - accuracy: 0.1839 - val_loss: 2.3198 - val_accuracy: 0.1920 - 5s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1250/1250 - 5s - loss: 2.3012 - accuracy: 0.2190 - val_loss: 2.2760 - val_accuracy: 0.2208 - 5s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1250/1250 - 5s - loss: 2.2811 - accuracy: 0.2322 - val_loss: 2.2555 - val_accuracy: 0.2411 - 5s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1250/1250 - 5s - loss: 2.2644 - accuracy: 0.2434 - val_loss: 2.2286 - val_accuracy: 0.2561 - 5s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1250/1250 - 5s - loss: 2.2584 - accuracy: 0.2483 - val_loss: 2.2041 - val_accuracy: 0.2842 - 5s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "1250/1250 - 5s - loss: 2.2557 - accuracy: 0.2485 - val_loss: 2.2427 - val_accuracy: 0.2376 - 5s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "1250/1250 - 5s - loss: 2.2460 - accuracy: 0.2536 - val_loss: 2.2607 - val_accuracy: 0.2478 - 5s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "1250/1250 - 5s - loss: 2.2439 - accuracy: 0.2521 - val_loss: 2.1911 - val_accuracy: 0.2775 - 5s/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "1250/1250 - 5s - loss: 2.2401 - accuracy: 0.2543 - val_loss: 2.1964 - val_accuracy: 0.2688 - 5s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "1250/1250 - 5s - loss: 2.2337 - accuracy: 0.2587 - val_loss: 2.1938 - val_accuracy: 0.2578 - 5s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "1250/1250 - 5s - loss: 2.2312 - accuracy: 0.2551 - val_loss: 2.1463 - val_accuracy: 0.2878 - 5s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "1250/1250 - 5s - loss: 2.2274 - accuracy: 0.2592 - val_loss: 2.1757 - val_accuracy: 0.2962 - 5s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "1250/1250 - 5s - loss: 2.2235 - accuracy: 0.2549 - val_loss: 2.2064 - val_accuracy: 0.2722 - 5s/epoch - 4ms/step\n",
            "Epoch 20/100\n",
            "1250/1250 - 5s - loss: 2.2196 - accuracy: 0.2581 - val_loss: 2.2205 - val_accuracy: 0.2795 - 5s/epoch - 4ms/step\n",
            "Epoch 21/100\n",
            "1250/1250 - 5s - loss: 2.2200 - accuracy: 0.2553 - val_loss: 2.1394 - val_accuracy: 0.2801 - 5s/epoch - 4ms/step\n",
            "Epoch 22/100\n",
            "1250/1250 - 5s - loss: 2.2140 - accuracy: 0.2608 - val_loss: 2.1313 - val_accuracy: 0.2922 - 5s/epoch - 4ms/step\n",
            "Epoch 23/100\n",
            "1250/1250 - 5s - loss: 2.2171 - accuracy: 0.2629 - val_loss: 2.1524 - val_accuracy: 0.2665 - 5s/epoch - 4ms/step\n",
            "Epoch 24/100\n",
            "1250/1250 - 6s - loss: 2.2130 - accuracy: 0.2663 - val_loss: 2.1223 - val_accuracy: 0.3021 - 6s/epoch - 4ms/step\n",
            "Epoch 25/100\n",
            "1250/1250 - 5s - loss: 2.2139 - accuracy: 0.2613 - val_loss: 2.1802 - val_accuracy: 0.2713 - 5s/epoch - 4ms/step\n",
            "Epoch 26/100\n",
            "1250/1250 - 5s - loss: 2.2050 - accuracy: 0.2680 - val_loss: 2.1412 - val_accuracy: 0.3043 - 5s/epoch - 4ms/step\n",
            "Epoch 27/100\n",
            "1250/1250 - 5s - loss: 2.2117 - accuracy: 0.2664 - val_loss: 2.1348 - val_accuracy: 0.3083 - 5s/epoch - 4ms/step\n",
            "Epoch 28/100\n",
            "1250/1250 - 5s - loss: 2.2107 - accuracy: 0.2659 - val_loss: 2.1117 - val_accuracy: 0.3041 - 5s/epoch - 4ms/step\n",
            "Epoch 29/100\n",
            "1250/1250 - 5s - loss: 2.2084 - accuracy: 0.2735 - val_loss: 2.1466 - val_accuracy: 0.2983 - 5s/epoch - 4ms/step\n",
            "Epoch 30/100\n",
            "1250/1250 - 5s - loss: 2.2027 - accuracy: 0.2738 - val_loss: 2.1205 - val_accuracy: 0.2834 - 5s/epoch - 4ms/step\n",
            "Epoch 31/100\n",
            "1250/1250 - 5s - loss: 2.2048 - accuracy: 0.2763 - val_loss: 2.1593 - val_accuracy: 0.2822 - 5s/epoch - 4ms/step\n",
            "Epoch 32/100\n",
            "1250/1250 - 5s - loss: 2.2046 - accuracy: 0.2771 - val_loss: 2.0817 - val_accuracy: 0.3318 - 5s/epoch - 4ms/step\n",
            "Epoch 33/100\n",
            "1250/1250 - 5s - loss: 2.2007 - accuracy: 0.2775 - val_loss: 2.0731 - val_accuracy: 0.2969 - 5s/epoch - 4ms/step\n",
            "Epoch 34/100\n",
            "1250/1250 - 5s - loss: 2.2042 - accuracy: 0.2791 - val_loss: 2.1276 - val_accuracy: 0.3054 - 5s/epoch - 4ms/step\n",
            "Epoch 35/100\n",
            "1250/1250 - 5s - loss: 2.2061 - accuracy: 0.2801 - val_loss: 2.1018 - val_accuracy: 0.3165 - 5s/epoch - 4ms/step\n",
            "Epoch 36/100\n",
            "1250/1250 - 5s - loss: 2.2043 - accuracy: 0.2786 - val_loss: 2.0743 - val_accuracy: 0.3491 - 5s/epoch - 4ms/step\n",
            "Epoch 37/100\n",
            "1250/1250 - 5s - loss: 2.1949 - accuracy: 0.2844 - val_loss: 2.1094 - val_accuracy: 0.3146 - 5s/epoch - 4ms/step\n",
            "Epoch 38/100\n",
            "1250/1250 - 5s - loss: 2.1956 - accuracy: 0.2834 - val_loss: 2.1183 - val_accuracy: 0.3117 - 5s/epoch - 4ms/step\n",
            "Epoch 39/100\n",
            "1250/1250 - 5s - loss: 2.1949 - accuracy: 0.2847 - val_loss: 2.1212 - val_accuracy: 0.2808 - 5s/epoch - 4ms/step\n",
            "Epoch 40/100\n",
            "1250/1250 - 5s - loss: 2.2006 - accuracy: 0.2836 - val_loss: 2.0966 - val_accuracy: 0.3343 - 5s/epoch - 4ms/step\n",
            "Epoch 41/100\n",
            "1250/1250 - 5s - loss: 2.1924 - accuracy: 0.2853 - val_loss: 2.1201 - val_accuracy: 0.3077 - 5s/epoch - 4ms/step\n",
            "Epoch 42/100\n",
            "1250/1250 - 6s - loss: 2.1943 - accuracy: 0.2849 - val_loss: 2.1290 - val_accuracy: 0.3285 - 6s/epoch - 4ms/step\n",
            "Epoch 43/100\n",
            "1250/1250 - 5s - loss: 2.1903 - accuracy: 0.2886 - val_loss: 2.1093 - val_accuracy: 0.3061 - 5s/epoch - 4ms/step\n",
            "Batch Size: 32, Learning Rate: 0.001, Test Accuracy: 0.3101\n",
            "Epoch 1/100\n",
            "1250/1250 - 8s - loss: 2.8174 - accuracy: 0.1488 - val_loss: 2.5705 - val_accuracy: 0.1792 - 8s/epoch - 6ms/step\n",
            "Epoch 2/100\n",
            "1250/1250 - 5s - loss: 2.6591 - accuracy: 0.2009 - val_loss: 2.7190 - val_accuracy: 0.2085 - 5s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "1250/1250 - 6s - loss: 2.7267 - accuracy: 0.2270 - val_loss: 2.9448 - val_accuracy: 0.1515 - 6s/epoch - 5ms/step\n",
            "Epoch 4/100\n",
            "1250/1250 - 5s - loss: 2.7653 - accuracy: 0.2357 - val_loss: 2.7085 - val_accuracy: 0.2492 - 5s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "1250/1250 - 5s - loss: 2.7544 - accuracy: 0.2377 - val_loss: 2.7044 - val_accuracy: 0.2509 - 5s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "1250/1250 - 5s - loss: 2.7650 - accuracy: 0.2409 - val_loss: 2.6748 - val_accuracy: 0.2476 - 5s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "1250/1250 - 6s - loss: 2.7642 - accuracy: 0.2403 - val_loss: 2.6378 - val_accuracy: 0.2575 - 6s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "1250/1250 - 5s - loss: 2.7665 - accuracy: 0.2378 - val_loss: 2.6616 - val_accuracy: 0.2944 - 5s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "1250/1250 - 5s - loss: 2.7630 - accuracy: 0.2385 - val_loss: 2.7089 - val_accuracy: 0.2538 - 5s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "1250/1250 - 5s - loss: 2.7648 - accuracy: 0.2370 - val_loss: 2.6857 - val_accuracy: 0.2383 - 5s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "1250/1250 - 5s - loss: 2.7780 - accuracy: 0.2355 - val_loss: 2.6257 - val_accuracy: 0.2754 - 5s/epoch - 4ms/step\n",
            "Batch Size: 32, Learning Rate: 0.01, Test Accuracy: 0.2768\n",
            "Epoch 1/100\n",
            "625/625 - 5s - loss: 6.4831 - accuracy: 0.1763 - val_loss: 2.3251 - val_accuracy: 0.0933 - 5s/epoch - 8ms/step\n",
            "Epoch 2/100\n",
            "625/625 - 3s - loss: 2.3246 - accuracy: 0.1126 - val_loss: 2.3245 - val_accuracy: 0.0933 - 3s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "625/625 - 2s - loss: 2.3248 - accuracy: 0.1182 - val_loss: 2.3248 - val_accuracy: 0.0933 - 2s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "625/625 - 2s - loss: 2.3250 - accuracy: 0.1183 - val_loss: 2.3248 - val_accuracy: 0.0933 - 2s/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "625/625 - 3s - loss: 2.3250 - accuracy: 0.1040 - val_loss: 2.3249 - val_accuracy: 0.1027 - 3s/epoch - 5ms/step\n",
            "Epoch 6/100\n",
            "625/625 - 3s - loss: 2.3249 - accuracy: 0.1246 - val_loss: 2.3240 - val_accuracy: 0.0911 - 3s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "625/625 - 2s - loss: 2.3250 - accuracy: 0.1244 - val_loss: 2.3236 - val_accuracy: 0.1572 - 2s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "625/625 - 3s - loss: 2.3251 - accuracy: 0.1320 - val_loss: 2.3255 - val_accuracy: 0.0980 - 3s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "625/625 - 2s - loss: 2.3250 - accuracy: 0.1440 - val_loss: 2.3248 - val_accuracy: 0.1572 - 2s/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "625/625 - 3s - loss: 2.3253 - accuracy: 0.1445 - val_loss: 2.3258 - val_accuracy: 0.1437 - 3s/epoch - 5ms/step\n",
            "Epoch 11/100\n",
            "625/625 - 2s - loss: 2.3259 - accuracy: 0.1570 - val_loss: 2.3257 - val_accuracy: 0.1584 - 2s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "625/625 - 3s - loss: 2.3256 - accuracy: 0.1817 - val_loss: 2.3299 - val_accuracy: 0.1868 - 3s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "625/625 - 2s - loss: 2.3169 - accuracy: 0.1845 - val_loss: 2.3127 - val_accuracy: 0.2296 - 2s/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "625/625 - 3s - loss: 2.2965 - accuracy: 0.2209 - val_loss: 2.2541 - val_accuracy: 0.2641 - 3s/epoch - 5ms/step\n",
            "Epoch 15/100\n",
            "625/625 - 2s - loss: 2.2638 - accuracy: 0.2434 - val_loss: 2.2201 - val_accuracy: 0.2645 - 2s/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "625/625 - 3s - loss: 2.2417 - accuracy: 0.2566 - val_loss: 2.1975 - val_accuracy: 0.3029 - 3s/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "625/625 - 3s - loss: 2.2247 - accuracy: 0.2682 - val_loss: 2.1897 - val_accuracy: 0.2968 - 3s/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "625/625 - 2s - loss: 2.2151 - accuracy: 0.2750 - val_loss: 2.1610 - val_accuracy: 0.2970 - 2s/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "625/625 - 3s - loss: 2.2041 - accuracy: 0.2777 - val_loss: 2.1894 - val_accuracy: 0.2794 - 3s/epoch - 5ms/step\n",
            "Epoch 20/100\n",
            "625/625 - 2s - loss: 2.1905 - accuracy: 0.2839 - val_loss: 2.1362 - val_accuracy: 0.3075 - 2s/epoch - 4ms/step\n",
            "Epoch 21/100\n",
            "625/625 - 3s - loss: 2.1867 - accuracy: 0.2910 - val_loss: 2.1354 - val_accuracy: 0.3056 - 3s/epoch - 4ms/step\n",
            "Epoch 22/100\n",
            "625/625 - 3s - loss: 2.1787 - accuracy: 0.2939 - val_loss: 2.1491 - val_accuracy: 0.3157 - 3s/epoch - 4ms/step\n",
            "Epoch 23/100\n",
            "625/625 - 3s - loss: 2.1702 - accuracy: 0.2993 - val_loss: 2.1285 - val_accuracy: 0.3296 - 3s/epoch - 5ms/step\n",
            "Epoch 24/100\n",
            "625/625 - 2s - loss: 2.1632 - accuracy: 0.3052 - val_loss: 2.1085 - val_accuracy: 0.3462 - 2s/epoch - 4ms/step\n",
            "Epoch 25/100\n",
            "625/625 - 2s - loss: 2.1550 - accuracy: 0.3094 - val_loss: 2.0766 - val_accuracy: 0.3540 - 2s/epoch - 4ms/step\n",
            "Epoch 26/100\n",
            "625/625 - 2s - loss: 2.1534 - accuracy: 0.3131 - val_loss: 2.0946 - val_accuracy: 0.3381 - 2s/epoch - 4ms/step\n",
            "Epoch 27/100\n",
            "625/625 - 3s - loss: 2.1436 - accuracy: 0.3227 - val_loss: 2.1141 - val_accuracy: 0.3551 - 3s/epoch - 4ms/step\n",
            "Epoch 28/100\n",
            "625/625 - 3s - loss: 2.1384 - accuracy: 0.3291 - val_loss: 2.0839 - val_accuracy: 0.3685 - 3s/epoch - 5ms/step\n",
            "Epoch 29/100\n",
            "625/625 - 3s - loss: 2.1278 - accuracy: 0.3353 - val_loss: 2.0606 - val_accuracy: 0.3902 - 3s/epoch - 4ms/step\n",
            "Epoch 30/100\n",
            "625/625 - 3s - loss: 2.1182 - accuracy: 0.3449 - val_loss: 2.0524 - val_accuracy: 0.3879 - 3s/epoch - 4ms/step\n",
            "Epoch 31/100\n",
            "625/625 - 3s - loss: 2.1095 - accuracy: 0.3456 - val_loss: 2.0465 - val_accuracy: 0.3864 - 3s/epoch - 4ms/step\n",
            "Epoch 32/100\n",
            "625/625 - 3s - loss: 2.1126 - accuracy: 0.3442 - val_loss: 2.0180 - val_accuracy: 0.4000 - 3s/epoch - 4ms/step\n",
            "Epoch 33/100\n",
            "625/625 - 3s - loss: 2.1003 - accuracy: 0.3427 - val_loss: 2.0398 - val_accuracy: 0.3844 - 3s/epoch - 4ms/step\n",
            "Epoch 34/100\n",
            "625/625 - 3s - loss: 2.0971 - accuracy: 0.3500 - val_loss: 2.0210 - val_accuracy: 0.3908 - 3s/epoch - 4ms/step\n",
            "Epoch 35/100\n",
            "625/625 - 3s - loss: 2.0946 - accuracy: 0.3521 - val_loss: 2.0006 - val_accuracy: 0.3824 - 3s/epoch - 4ms/step\n",
            "Epoch 36/100\n",
            "625/625 - 3s - loss: 2.0891 - accuracy: 0.3499 - val_loss: 1.9896 - val_accuracy: 0.3987 - 3s/epoch - 4ms/step\n",
            "Epoch 37/100\n",
            "625/625 - 3s - loss: 2.0908 - accuracy: 0.3495 - val_loss: 2.0143 - val_accuracy: 0.3827 - 3s/epoch - 5ms/step\n",
            "Epoch 38/100\n",
            "625/625 - 3s - loss: 2.0837 - accuracy: 0.3538 - val_loss: 2.0237 - val_accuracy: 0.4040 - 3s/epoch - 5ms/step\n",
            "Epoch 39/100\n",
            "625/625 - 3s - loss: 2.0894 - accuracy: 0.3486 - val_loss: 2.0063 - val_accuracy: 0.3919 - 3s/epoch - 5ms/step\n",
            "Epoch 40/100\n",
            "625/625 - 3s - loss: 2.0800 - accuracy: 0.3565 - val_loss: 1.9981 - val_accuracy: 0.4018 - 3s/epoch - 4ms/step\n",
            "Epoch 41/100\n",
            "625/625 - 3s - loss: 2.0724 - accuracy: 0.3563 - val_loss: 2.0010 - val_accuracy: 0.4006 - 3s/epoch - 5ms/step\n",
            "Epoch 42/100\n",
            "625/625 - 3s - loss: 2.0722 - accuracy: 0.3589 - val_loss: 1.9737 - val_accuracy: 0.4095 - 3s/epoch - 4ms/step\n",
            "Epoch 43/100\n",
            "625/625 - 3s - loss: 2.0705 - accuracy: 0.3600 - val_loss: 1.9651 - val_accuracy: 0.4144 - 3s/epoch - 4ms/step\n",
            "Epoch 44/100\n",
            "625/625 - 2s - loss: 2.0651 - accuracy: 0.3557 - val_loss: 1.9595 - val_accuracy: 0.4145 - 2s/epoch - 4ms/step\n",
            "Epoch 45/100\n",
            "625/625 - 3s - loss: 2.0641 - accuracy: 0.3580 - val_loss: 1.9824 - val_accuracy: 0.4034 - 3s/epoch - 5ms/step\n",
            "Epoch 46/100\n",
            "625/625 - 3s - loss: 2.0691 - accuracy: 0.3548 - val_loss: 1.9578 - val_accuracy: 0.4145 - 3s/epoch - 5ms/step\n",
            "Epoch 47/100\n",
            "625/625 - 3s - loss: 2.0604 - accuracy: 0.3594 - val_loss: 1.9820 - val_accuracy: 0.3791 - 3s/epoch - 4ms/step\n",
            "Epoch 48/100\n",
            "625/625 - 2s - loss: 2.0523 - accuracy: 0.3641 - val_loss: 2.0028 - val_accuracy: 0.3720 - 2s/epoch - 4ms/step\n",
            "Epoch 49/100\n",
            "625/625 - 2s - loss: 2.0538 - accuracy: 0.3634 - val_loss: 1.9708 - val_accuracy: 0.3899 - 2s/epoch - 4ms/step\n",
            "Epoch 50/100\n",
            "625/625 - 3s - loss: 2.0559 - accuracy: 0.3622 - val_loss: 1.9801 - val_accuracy: 0.3949 - 3s/epoch - 5ms/step\n",
            "Epoch 51/100\n",
            "625/625 - 3s - loss: 2.0570 - accuracy: 0.3587 - val_loss: 1.9878 - val_accuracy: 0.4021 - 3s/epoch - 4ms/step\n",
            "Epoch 52/100\n",
            "625/625 - 3s - loss: 2.0555 - accuracy: 0.3578 - val_loss: 1.9442 - val_accuracy: 0.4203 - 3s/epoch - 5ms/step\n",
            "Epoch 53/100\n",
            "625/625 - 3s - loss: 2.0526 - accuracy: 0.3633 - val_loss: 1.9714 - val_accuracy: 0.3628 - 3s/epoch - 4ms/step\n",
            "Epoch 54/100\n",
            "625/625 - 3s - loss: 2.0557 - accuracy: 0.3600 - val_loss: 1.9325 - val_accuracy: 0.4169 - 3s/epoch - 4ms/step\n",
            "Epoch 55/100\n",
            "625/625 - 3s - loss: 2.0561 - accuracy: 0.3578 - val_loss: 1.9363 - val_accuracy: 0.4118 - 3s/epoch - 5ms/step\n",
            "Epoch 56/100\n",
            "625/625 - 3s - loss: 2.0528 - accuracy: 0.3587 - val_loss: 1.9469 - val_accuracy: 0.4134 - 3s/epoch - 4ms/step\n",
            "Epoch 57/100\n",
            "625/625 - 3s - loss: 2.0467 - accuracy: 0.3663 - val_loss: 1.9426 - val_accuracy: 0.4170 - 3s/epoch - 5ms/step\n",
            "Epoch 58/100\n",
            "625/625 - 3s - loss: 2.0439 - accuracy: 0.3635 - val_loss: 1.9338 - val_accuracy: 0.4135 - 3s/epoch - 4ms/step\n",
            "Epoch 59/100\n",
            "625/625 - 3s - loss: 2.0448 - accuracy: 0.3632 - val_loss: 1.9396 - val_accuracy: 0.4099 - 3s/epoch - 5ms/step\n",
            "Epoch 60/100\n",
            "625/625 - 2s - loss: 2.0367 - accuracy: 0.3677 - val_loss: 1.9335 - val_accuracy: 0.4074 - 2s/epoch - 4ms/step\n",
            "Epoch 61/100\n",
            "625/625 - 3s - loss: 2.0466 - accuracy: 0.3617 - val_loss: 1.9288 - val_accuracy: 0.4142 - 3s/epoch - 4ms/step\n",
            "Epoch 62/100\n",
            "625/625 - 3s - loss: 2.0509 - accuracy: 0.3608 - val_loss: 1.9664 - val_accuracy: 0.3912 - 3s/epoch - 4ms/step\n",
            "Epoch 63/100\n",
            "625/625 - 3s - loss: 2.0478 - accuracy: 0.3622 - val_loss: 1.9548 - val_accuracy: 0.3932 - 3s/epoch - 5ms/step\n",
            "Epoch 64/100\n",
            "625/625 - 3s - loss: 2.0355 - accuracy: 0.3634 - val_loss: 1.9857 - val_accuracy: 0.3770 - 3s/epoch - 4ms/step\n",
            "Epoch 65/100\n",
            "625/625 - 2s - loss: 2.0330 - accuracy: 0.3688 - val_loss: 1.9193 - val_accuracy: 0.4263 - 2s/epoch - 4ms/step\n",
            "Epoch 66/100\n",
            "625/625 - 2s - loss: 2.0402 - accuracy: 0.3637 - val_loss: 1.9093 - val_accuracy: 0.4327 - 2s/epoch - 4ms/step\n",
            "Epoch 67/100\n",
            "625/625 - 3s - loss: 2.0421 - accuracy: 0.3634 - val_loss: 1.9043 - val_accuracy: 0.4181 - 3s/epoch - 4ms/step\n",
            "Epoch 68/100\n",
            "625/625 - 3s - loss: 2.0354 - accuracy: 0.3683 - val_loss: 1.9313 - val_accuracy: 0.4074 - 3s/epoch - 5ms/step\n",
            "Epoch 69/100\n",
            "625/625 - 3s - loss: 2.0352 - accuracy: 0.3624 - val_loss: 1.9275 - val_accuracy: 0.4174 - 3s/epoch - 4ms/step\n",
            "Epoch 70/100\n",
            "625/625 - 3s - loss: 2.0334 - accuracy: 0.3665 - val_loss: 1.9104 - val_accuracy: 0.4224 - 3s/epoch - 4ms/step\n",
            "Epoch 71/100\n",
            "625/625 - 3s - loss: 2.0310 - accuracy: 0.3702 - val_loss: 1.9210 - val_accuracy: 0.4224 - 3s/epoch - 4ms/step\n",
            "Epoch 72/100\n",
            "625/625 - 3s - loss: 2.0348 - accuracy: 0.3616 - val_loss: 1.9465 - val_accuracy: 0.4123 - 3s/epoch - 4ms/step\n",
            "Epoch 73/100\n",
            "625/625 - 3s - loss: 2.0367 - accuracy: 0.3651 - val_loss: 1.9039 - val_accuracy: 0.4002 - 3s/epoch - 4ms/step\n",
            "Epoch 74/100\n",
            "625/625 - 3s - loss: 2.0336 - accuracy: 0.3628 - val_loss: 1.9199 - val_accuracy: 0.4139 - 3s/epoch - 4ms/step\n",
            "Epoch 75/100\n",
            "625/625 - 2s - loss: 2.0340 - accuracy: 0.3654 - val_loss: 1.9356 - val_accuracy: 0.4177 - 2s/epoch - 4ms/step\n",
            "Epoch 76/100\n",
            "625/625 - 2s - loss: 2.0402 - accuracy: 0.3661 - val_loss: 1.9528 - val_accuracy: 0.3947 - 2s/epoch - 4ms/step\n",
            "Epoch 77/100\n",
            "625/625 - 3s - loss: 2.0382 - accuracy: 0.3663 - val_loss: 1.9014 - val_accuracy: 0.4353 - 3s/epoch - 5ms/step\n",
            "Epoch 78/100\n",
            "625/625 - 3s - loss: 2.0366 - accuracy: 0.3620 - val_loss: 1.9049 - val_accuracy: 0.4137 - 3s/epoch - 4ms/step\n",
            "Epoch 79/100\n",
            "625/625 - 2s - loss: 2.0302 - accuracy: 0.3684 - val_loss: 1.9412 - val_accuracy: 0.3925 - 2s/epoch - 4ms/step\n",
            "Epoch 80/100\n",
            "625/625 - 2s - loss: 2.0275 - accuracy: 0.3708 - val_loss: 1.9129 - val_accuracy: 0.4079 - 2s/epoch - 4ms/step\n",
            "Epoch 81/100\n",
            "625/625 - 3s - loss: 2.0313 - accuracy: 0.3663 - val_loss: 1.9248 - val_accuracy: 0.4026 - 3s/epoch - 4ms/step\n",
            "Epoch 82/100\n",
            "625/625 - 3s - loss: 2.0335 - accuracy: 0.3665 - val_loss: 1.8829 - val_accuracy: 0.4129 - 3s/epoch - 5ms/step\n",
            "Epoch 83/100\n",
            "625/625 - 2s - loss: 2.0220 - accuracy: 0.3667 - val_loss: 1.9243 - val_accuracy: 0.4041 - 2s/epoch - 4ms/step\n",
            "Epoch 84/100\n",
            "625/625 - 3s - loss: 2.0263 - accuracy: 0.3693 - val_loss: 1.9216 - val_accuracy: 0.4174 - 3s/epoch - 4ms/step\n",
            "Epoch 85/100\n",
            "625/625 - 3s - loss: 2.0248 - accuracy: 0.3711 - val_loss: 1.9185 - val_accuracy: 0.4062 - 3s/epoch - 5ms/step\n",
            "Epoch 86/100\n",
            "625/625 - 3s - loss: 2.0257 - accuracy: 0.3656 - val_loss: 1.8826 - val_accuracy: 0.4228 - 3s/epoch - 5ms/step\n",
            "Epoch 87/100\n",
            "625/625 - 3s - loss: 2.0326 - accuracy: 0.3645 - val_loss: 1.9069 - val_accuracy: 0.4320 - 3s/epoch - 4ms/step\n",
            "Epoch 88/100\n",
            "625/625 - 2s - loss: 2.0304 - accuracy: 0.3652 - val_loss: 1.9180 - val_accuracy: 0.4096 - 2s/epoch - 4ms/step\n",
            "Epoch 89/100\n",
            "625/625 - 3s - loss: 2.0249 - accuracy: 0.3677 - val_loss: 1.9255 - val_accuracy: 0.4001 - 3s/epoch - 4ms/step\n",
            "Epoch 90/100\n",
            "625/625 - 3s - loss: 2.0307 - accuracy: 0.3675 - val_loss: 1.9094 - val_accuracy: 0.4122 - 3s/epoch - 4ms/step\n",
            "Epoch 91/100\n",
            "625/625 - 3s - loss: 2.0175 - accuracy: 0.3704 - val_loss: 1.9154 - val_accuracy: 0.4130 - 3s/epoch - 5ms/step\n",
            "Epoch 92/100\n",
            "625/625 - 3s - loss: 2.0330 - accuracy: 0.3632 - val_loss: 1.9096 - val_accuracy: 0.4259 - 3s/epoch - 4ms/step\n",
            "Epoch 93/100\n",
            "625/625 - 3s - loss: 2.0210 - accuracy: 0.3686 - val_loss: 1.9032 - val_accuracy: 0.4217 - 3s/epoch - 4ms/step\n",
            "Epoch 94/100\n",
            "625/625 - 3s - loss: 2.0251 - accuracy: 0.3660 - val_loss: 1.9152 - val_accuracy: 0.4232 - 3s/epoch - 5ms/step\n",
            "Epoch 95/100\n",
            "625/625 - 3s - loss: 2.0257 - accuracy: 0.3700 - val_loss: 1.8874 - val_accuracy: 0.4268 - 3s/epoch - 5ms/step\n",
            "Epoch 96/100\n",
            "625/625 - 2s - loss: 2.0277 - accuracy: 0.3683 - val_loss: 1.9350 - val_accuracy: 0.3908 - 2s/epoch - 4ms/step\n",
            "Batch Size: 64, Learning Rate: 0.001, Test Accuracy: 0.3917\n",
            "Epoch 1/100\n",
            "625/625 - 57s - loss: 3.0163 - accuracy: 0.1379 - val_loss: 2.5336 - val_accuracy: 0.1453 - 57s/epoch - 92ms/step\n",
            "Epoch 2/100\n",
            "625/625 - 3s - loss: 2.5389 - accuracy: 0.1893 - val_loss: 2.5105 - val_accuracy: 0.2225 - 3s/epoch - 4ms/step\n",
            "Epoch 3/100\n",
            "625/625 - 2s - loss: 2.5599 - accuracy: 0.2243 - val_loss: 2.4540 - val_accuracy: 0.2645 - 2s/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "625/625 - 3s - loss: 2.5662 - accuracy: 0.2320 - val_loss: 2.4876 - val_accuracy: 0.2369 - 3s/epoch - 5ms/step\n",
            "Epoch 5/100\n",
            "625/625 - 3s - loss: 2.5774 - accuracy: 0.2476 - val_loss: 2.6812 - val_accuracy: 0.1939 - 3s/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "625/625 - 3s - loss: 2.5996 - accuracy: 0.2464 - val_loss: 2.4870 - val_accuracy: 0.2784 - 3s/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "625/625 - 2s - loss: 2.5981 - accuracy: 0.2501 - val_loss: 2.6585 - val_accuracy: 0.2318 - 2s/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "625/625 - 3s - loss: 2.6068 - accuracy: 0.2503 - val_loss: 2.5516 - val_accuracy: 0.2411 - 3s/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "625/625 - 3s - loss: 2.6066 - accuracy: 0.2547 - val_loss: 2.5591 - val_accuracy: 0.2598 - 3s/epoch - 5ms/step\n",
            "Epoch 10/100\n",
            "625/625 - 3s - loss: 2.6200 - accuracy: 0.2494 - val_loss: 2.5063 - val_accuracy: 0.2675 - 3s/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "625/625 - 3s - loss: 2.6090 - accuracy: 0.2502 - val_loss: 2.5517 - val_accuracy: 0.2365 - 3s/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "625/625 - 3s - loss: 2.6212 - accuracy: 0.2449 - val_loss: 2.6326 - val_accuracy: 0.2528 - 3s/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "625/625 - 3s - loss: 2.6134 - accuracy: 0.2440 - val_loss: 2.5118 - val_accuracy: 0.2602 - 3s/epoch - 5ms/step\n",
            "Batch Size: 64, Learning Rate: 0.01, Test Accuracy: 0.2695\n",
            "Best Accuracy: 0.3917 with Batch Size: 64, Learning Rate: 0.001\n"
          ]
        }
      ],
      "source": [
        "for batch_size in batch_sizes:\n",
        "    for lr in learning_rates:\n",
        "        # MLP for recognition with Elastic Net regularization\n",
        "        mlp = Sequential([\n",
        "            Dense(400, activation='relu', input_shape=(flatten_train_np.shape[1],)),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.5),\n",
        "            Dense(64, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.5),\n",
        "            Dense(10, activation='softmax', kernel_regularizer=l1_l2(l1=rho * alpha, l2=(1 - rho) * alpha))\n",
        "        ])\n",
        "\n",
        "        # Compile the MLP\n",
        "        optimizer = Adam(learning_rate=lr)\n",
        "        mlp.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        # Train the MLP on the encoded features\n",
        "        x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(\n",
        "            flatten_train_np, y_train, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        early_stopping_monitor = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
        "        mlp_history = mlp.fit(x_train_split, y_train_split,\n",
        "                              epochs=100,\n",
        "                              batch_size=batch_size,\n",
        "                              validation_data=(x_val_split, y_val_split),\n",
        "                              callbacks=[early_stopping_monitor],\n",
        "                              verbose=2)\n",
        "\n",
        "        # Evaluate MLP on test set\n",
        "        test_loss, test_accuracy = mlp.evaluate(flatten_test_np, y_test, verbose=0)\n",
        "\n",
        "        # Store results\n",
        "        if test_accuracy > best_accuracy:\n",
        "            best_accuracy = test_accuracy\n",
        "            best_params = {'Batch Size': batch_size, 'Learning Rate': lr}\n",
        "\n",
        "        print(f'Batch Size: {batch_size}, Learning Rate: {lr}, Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "print(f'Best Accuracy: {best_accuracy:.4f} with Batch Size: {best_params[\"Batch Size\"]}, Learning Rate: {best_params[\"Learning Rate\"]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDtVAoHaSIP_"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        " Best Accuracy: 0.3917 with Batch Size: 64, Learning Rate: 0.001\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6ZLD5reTReP"
      },
      "source": [
        "## Grid search is a systematic method, but it can be computationally expensive, especially when dealing with a large number of hyperparameters and their possible values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3PjQY72FuCi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIWHaiuPRhSW"
      },
      "source": [
        "***Approach 3:Bayesian***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "de9r1H1tjNCU",
        "outputId": "5ec410d5-e86f-4d75-965e-acefd570cf00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1563/1563 [==============================] - 17s 10ms/step\n",
            "313/313 [==============================] - 3s 10ms/step\n",
            "100%|| 20/20 [38:45<00:00, 116.29s/trial, best loss: -0.26100000739097595]\n",
            "Best hyperparameters: Batch Size: 32, Learning Rate: 0.009360527227641056, Alpha: 0.6919125492039278, Rho: 0.18166965893871023\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.regularizers import l1_l2\n",
        "from hyperopt import fmin, tpe, hp\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize and convert to float32\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Define the input shape\n",
        "input_img = Input(shape=(32, 32, 3))\n",
        "\n",
        "# Encoder part of the CAE\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "# Flatten the encoded output for MLP\n",
        "flatten = Flatten()(encoded)\n",
        "\n",
        "# Create a model for encoding features\n",
        "encoder_model = Model(input_img, flatten)\n",
        "\n",
        "# Obtain encoded features for training and validation sets\n",
        "flatten_train_np = encoder_model.predict(x_train)\n",
        "flatten_test_np = encoder_model.predict(x_test)\n",
        "\n",
        "# Define the objective function for Bayesian optimization\n",
        "def objective(params, flatten_train_np=flatten_train_np, y_train=y_train, flatten_test_np=flatten_test_np, y_test=y_test):\n",
        "    # Extract hyperparameters\n",
        "    batch_size = int(params['batch_size'])\n",
        "    lr = params['lr']\n",
        "\n",
        "    # MLP for recognition with Elastic Net regularization\n",
        "    mlp = Sequential([\n",
        "        Dense(400, activation='relu', input_shape=(flatten_train_np.shape[1],)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(64, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(10, activation='softmax', kernel_regularizer=l1_l2(l1=params['rho'] * params['alpha'], l2=(1 - params['rho']) * params['alpha']))\n",
        "    ])\n",
        "\n",
        "    # Compile the MLP\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    mlp.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the MLP on the encoded features\n",
        "    x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(\n",
        "        flatten_train_np, y_train, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    early_stopping_monitor = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
        "    mlp_history = mlp.fit(x_train_split, y_train_split,\n",
        "                          epochs=100,\n",
        "                          batch_size=batch_size,\n",
        "                          validation_data=(x_val_split, y_val_split),\n",
        "                          callbacks=[early_stopping_monitor],\n",
        "                          verbose=0)\n",
        "\n",
        "    # Evaluate MLP on test set\n",
        "    test_loss, test_accuracy = mlp.evaluate(flatten_test_np, y_test, verbose=0)\n",
        "\n",
        "    # Return the negative accuracy (as hyperopt minimizes the objective)\n",
        "    return -test_accuracy\n",
        "\n",
        "# Define the hyperparameter space for Bayesian optimization\n",
        "space = {\n",
        "    'batch_size': hp.choice('batch_size', [8, 16, 32, 64]),\n",
        "    'lr': hp.loguniform('lr', -5, 0),  # Learning rate in log scale\n",
        "    'alpha': hp.uniform('alpha', 0, 1),\n",
        "    'rho': hp.uniform('rho', 0, 1)\n",
        "}\n",
        "\n",
        "# Perform Bayesian optimization\n",
        "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=20)\n",
        "\n",
        "# Extract the best hyperparameters\n",
        "best_batch_size = [8, 16, 32, 64][best['batch_size']]\n",
        "best_lr = best['lr']\n",
        "best_alpha = best['alpha']\n",
        "best_rho = best['rho']\n",
        "\n",
        "print(f'Best hyperparameters: Batch Size: {best_batch_size}, Learning Rate: {best_lr}, Alpha: {best_alpha}, Rho: {best_rho}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUP41y9Se6gh"
      },
      "source": [
        "with more polishing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "tc-TnxhlDpmE",
        "outputId": "0023a68b-6dba-48cb-9b5c-0e902c9b6a49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1563/1563 [==============================] - 17s 11ms/step\n",
            "313/313 [==============================] - 3s 10ms/step\n",
            "100%|| 20/20 [31:04<00:00, 93.24s/trial, best loss: -0.38760000467300415]\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "1",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-076ca0234e9e>\u001b[0m in \u001b[0;36m<cell line: 97>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# Retrieve the best MLP model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m# Display sample predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1"
          ]
        }
      ],
      "source": [
        "\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.regularizers import l1_l2\n",
        "from hyperopt import fmin, tpe, hp\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize and convert to float32\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Define the input shape\n",
        "input_img = Input(shape=(32, 32, 3))\n",
        "\n",
        "# Encoder part of the CAE\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "# Flatten the encoded output for MLP\n",
        "flatten = Flatten()(encoded)\n",
        "\n",
        "# Create a model for encoding features\n",
        "encoder_model = Model(input_img, flatten)\n",
        "\n",
        "# Obtain encoded features for training and validation sets\n",
        "flatten_train_np = encoder_model.predict(x_train)\n",
        "flatten_test_np = encoder_model.predict(x_test)\n",
        "\n",
        "# Define the objective function for Bayesian optimization\n",
        "def objective(params, flatten_train_np=flatten_train_np, y_train=y_train, flatten_test_np=flatten_test_np, y_test=y_test):\n",
        "    # Extract hyperparameters\n",
        "    batch_size = int(params['batch_size'])\n",
        "    lr = params['lr']\n",
        "\n",
        "    # MLP for recognition with Elastic Net regularization\n",
        "    mlp = Sequential([\n",
        "        Dense(400, activation='relu', input_shape=(flatten_train_np.shape[1],)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(64, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(10, activation='softmax', kernel_regularizer=l1_l2(l1=params['rho'] * params['alpha'], l2=(1 - params['rho']) * params['alpha']))\n",
        "    ])\n",
        "\n",
        "    # Compile the MLP\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    mlp.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the MLP on the encoded features\n",
        "    x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(\n",
        "        flatten_train_np, y_train, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    early_stopping_monitor = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
        "    mlp_history = mlp.fit(x_train_split, y_train_split,\n",
        "                          epochs=100,\n",
        "                          batch_size=batch_size,\n",
        "                          validation_data=(x_val_split, y_val_split),\n",
        "                          callbacks=[early_stopping_monitor],\n",
        "                          verbose=0)\n",
        "\n",
        "    # Evaluate MLP on test set\n",
        "    test_loss, test_accuracy = mlp.evaluate(flatten_test_np, y_test, verbose=0)\n",
        "\n",
        "    # Return a dictionary with 'loss' and 'status' keys\n",
        "    return {'loss': -test_accuracy, 'status': 'ok', 'model': mlp}\n",
        "\n",
        "# Define the hyperparameter space for Bayesian optimization\n",
        "space = {\n",
        "    'batch_size': hp.choice('batch_size', [8, 16, 32, 64]),\n",
        "    'lr': hp.loguniform('lr', -5, 0),  # Learning rate in log scale\n",
        "    'alpha': hp.uniform('alpha', 0, 1),\n",
        "    'rho': hp.uniform('rho', 0, 1)\n",
        "}\n",
        "\n",
        "# Perform Bayesian optimization\n",
        "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=20)\n",
        "\n",
        "# Extract the best hyperparameters\n",
        "best_batch_size = [8, 16, 32, 64][best['batch_size']]\n",
        "best_lr = best['lr']\n",
        "best_alpha = best['alpha']\n",
        "best_rho = best['rho']\n",
        "\n",
        "# Retrieve the best MLP model\n",
        "best_model = objective(best)[1]\n",
        "\n",
        "# Display sample predictions\n",
        "sample_indices = np.random.choice(len(x_test), 5, replace=False)\n",
        "sample_images = x_test[sample_indices]\n",
        "sample_labels = y_test[sample_indices]\n",
        "\n",
        "predictions = best_model.predict(encoder_model.predict(sample_images))\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "print(\"\\nSample Predictions:\")\n",
        "for i in range(len(sample_images)):\n",
        "    print(f\"Actual Label: {sample_labels[i]}, Predicted Label: {predicted_labels[i]}\")\n",
        "\n",
        "# Display the accuracy\n",
        "print(f\"\\nBest Accuracy: {-best['loss']:.4f} with Batch Size: {best_batch_size}, Learning Rate: {best_lr}, Alpha: {best_alpha}, Rho: {best_rho}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MhgcXPcSYTZ"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# 100%|| 20/20 [44:06<00:00, 132.30s/trial, best loss: -0.28060001134872437]\n",
        "Best hyperparameters: Batch Size: 64, Learning Rate: 0.01763211249105561, Alpha: 0.8638905622827661, Rho: 0.11757792046491378\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itu6oE0uiJ6Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ba4e32d-7a3e-42c6-ee44-37fbc39d4390"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 601/1563 [==========>...................] - ETA: 11s"
          ]
        }
      ],
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.regularizers import l1_l2\n",
        "from hyperopt import fmin, tpe, hp\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize and convert to float32\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Define the input shape\n",
        "input_img = Input(shape=(32, 32, 3))\n",
        "\n",
        "# Encoder part of the CAE\n",
        "x = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "# Flatten the encoded output for MLP\n",
        "flatten = Flatten()(encoded)\n",
        "\n",
        "# Create a model for encoding features\n",
        "encoder_model = Model(input_img, flatten)\n",
        "\n",
        "# Obtain encoded features for training and validation sets\n",
        "flatten_train_np = encoder_model.predict(x_train)\n",
        "flatten_test_np = encoder_model.predict(x_test)\n",
        "\n",
        "# Define the objective function for Bayesian optimization\n",
        "def objective(params, flatten_train_np=flatten_train_np, y_train=y_train, flatten_test_np=flatten_test_np, y_test=y_test):\n",
        "    # Extract hyperparameters\n",
        "    batch_size = int(params['batch_size'])\n",
        "    lr = params['lr']\n",
        "\n",
        "    # MLP for recognition with Elastic Net regularization\n",
        "    mlp = Sequential([\n",
        "        Dense(400, activation='relu', input_shape=(flatten_train_np.shape[1],)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(64, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "        Dense(10, activation='softmax', kernel_regularizer=l1_l2(l1=params['rho'] * params['alpha'], l2=(1 - params['rho']) * params['alpha']))\n",
        "    ])\n",
        "\n",
        "    # Compile the MLP\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "    mlp.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the MLP on the encoded features\n",
        "    x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(\n",
        "        flatten_train_np, y_train, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    early_stopping_monitor = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
        "    mlp_history = mlp.fit(x_train_split, y_train_split,\n",
        "                          epochs=100,\n",
        "                          batch_size=batch_size,\n",
        "                          validation_data=(x_val_split, y_val_split),\n",
        "                          callbacks=[early_stopping_monitor],\n",
        "                          verbose=0)\n",
        "\n",
        "    # Evaluate MLP on test set\n",
        "    test_loss, test_accuracy = mlp.evaluate(flatten_test_np, y_test, verbose=0)\n",
        "\n",
        "    # Return a dictionary with 'loss' and 'status' keys\n",
        "    return {'loss': -test_accuracy, 'status': 'ok', 'accuracy': test_accuracy}\n",
        "\n",
        "# Define the hyperparameter space for Bayesian optimization\n",
        "space = {\n",
        "    'batch_size': hp.choice('batch_size', [8, 16, 32, 64]),\n",
        "    'lr': hp.loguniform('lr', -5, 0),  # Learning rate in log scale\n",
        "    'alpha': hp.uniform('alpha', 0, 1),\n",
        "    'rho': hp.uniform('rho', 0, 1)\n",
        "}\n",
        "\n",
        "# Perform Bayesian optimization\n",
        "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=20)\n",
        "\n",
        "# Extract the best hyperparameters\n",
        "best_batch_size = [8, 16, 32, 64][best['batch_size']]\n",
        "best_lr = best['lr']\n",
        "best_alpha = best['alpha']\n",
        "best_rho = best['rho']\n",
        "best_accuracy = best['accuracy']\n",
        "\n",
        "print(f'Best hyperparameters: Batch Size: {best_batch_size}, Learning Rate: {best_lr}, Alpha: {best_alpha}, Rho: {best_rho}')\n",
        "print(f'Best Accuracy: {best_accuracy}')\n",
        "\n",
        "# Retrieve the best MLP model\n",
        "best_model = objective(best)[1]\n",
        "\n",
        "# Display sample predictions\n",
        "sample_indices = np.random.choice(len(x_test), 5, replace=False)\n",
        "sample_images = x_test[sample_indices]\n",
        "sample_labels = y_test[sample_indices]\n",
        "\n",
        "predictions = best_model.predict(encoder_model.predict(sample_images))\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "print(\"\\nSample Predictions:\")\n",
        "for i in range(len(sample_images)):\n",
        "    print(f\"Actual Label: {sample_labels[i]}, Predicted Label: {predicted_labels[i]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhlXL2Jse6HV"
      },
      "source": [
        "## Using with MStar dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYpJSD3kfEn4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}